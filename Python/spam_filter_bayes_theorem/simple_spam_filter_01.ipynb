{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.4"
    },
    "colab": {
      "name": "simple_spam_filter.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xFsBNO_jGzD"
      },
      "source": [
        "<h1>SIMPLE SPAM FILTER</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AxXaeTyjGzJ"
      },
      "source": [
        "######This is my first raw machine learning coding project. Although it is nothing compare to others' spam filter projects, I am still proud of it. Just do not consider it to be a good reference on your study :>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pDnBQeQjGzM"
      },
      "source": [
        "######First, I want to talk about the mathematics here before we start coding. This spam filter is based on the Bayes theorem, as we have learned that:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCds_16fjGzN"
      },
      "source": [
        "<code><bold>P(spam|word) = P(word|spam)*P(spam)/P(word)</bold></code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wv1bXx6jGzO"
      },
      "source": [
        "######(Because this is simple, the only feature I take into account for predicting spam is the word, including words in title and email address.) We have the formula of predicting an email spam or ham as below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JLF4hnjGzQ"
      },
      "source": [
        "<code><bold>P(spam|w_1, w_2,..., w_n) = P(w_1, w_2,..., w_n|spam)\\*P(w_1, w_2,..., w_n)/P(spam)</bold></code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUWcMABKjGzR"
      },
      "source": [
        "((w_1, w_2,..., w_n) belong to the W set). Based on the Chain Rule, we have:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA-oP4MrjGzS"
      },
      "source": [
        "<code><bold>P(w_1, w_2,..., w_n) = P(w_1|w_2,..., w_n)\\*P(w_2|w_3,..., w_n)\\*...\\*P(w_n)</bold></code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmyXScIrjGzU"
      },
      "source": [
        "######To make it simple, let's suppose that every word in the W set is mutually independent on the category \"spam\" or \"ham\" only (this is the \"Naive Bayes\" assumption), then we have:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gO_F_00jGzW"
      },
      "source": [
        "<code><bold>P(w_1, w_2,..., w_n|spam) = Pi\\<i = 1 -> n>(P(w_i|spam))</bold></code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TllAJT4jGzX"
      },
      "source": [
        "######Therefore:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmEkGl47jGzY"
      },
      "source": [
        "<code><bold>P(spam|w_1, w_2,..., w_n) = P(w_1, w_2,..., w_n|spam)\\*P(spam)/P(w_1, w_2,..., w_n)</bold></code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSfz3wljjGzZ"
      },
      "source": [
        "<code><bold>P(spam|w_1, w_2,..., w_n) = Pi\\<i = 1 -> n>(P(w_i|spam))\\*P(spam)/P(w_1, w_2,..., w_n)</bold></code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8vBbHNrjGza"
      },
      "source": [
        "In this step, I am actually confused in figuring out how to calculate the P(w_1, w_2,..., w_n), so I ask for advice from my brilliant friend and he propose another solution. I will get the ratio of P(ham|w_1, w_2,..., w_n) over P(spam|w_1, w_2,..., w_n):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nknGjtAFjGzb"
      },
      "source": [
        "<code><bold>R = P(ham|w_1, w_2,..., w_n)/P(spam|w_1, w_2,..., w_n)</bold></code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPdajF47jGzb"
      },
      "source": [
        "<code><bold>R = Pi\\<i = 1 -> n>(P(w_i|spam))\\*P(spam)/Pi\\<i = 1 -> n>(P(w_i|ham))\\*P(ham)</bold></code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd1qjPyKjGzc"
      },
      "source": [
        "Now it is easy to calculate R, is it? Let's go back to the P(spam|w_1, w_2,..., w_n):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hft13bxhjGzd"
      },
      "source": [
        "<code><bold>P(spam|w_1, w_2,..., w_n) = P(spam|w_1, w_2,... w_n)/(P(spam|w_1, w_2,..., w_n)\\*P(ham|w_1, w_2,..., w_n))</bold></code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K75P06yjjGze"
      },
      "source": [
        "<code><bold>P(spam|w_1, w_2,..., w_n) = 1/(1 + (P(ham|w_1, w_2,..., w_n)/P(spam|w_1, w_2,..., w_n)))</bold></code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLXd8Xp7jGzf"
      },
      "source": [
        "<code><bold>P(spam|w_1, w_2,..., w_n) = 1/(1 + R)</bold></code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSzERie3jGzg"
      },
      "source": [
        "Now we can get started in coding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzZwXKVSjGzh",
        "outputId": "cf7a7fa7-334a-4c9d-b50c-d513bcb70396"
      },
      "source": [
        "# import necessary modules\n",
        "import pandas as pd\n",
        "import re, math\n",
        "# get the dataset\n",
        "fraud_email = pd.read_csv('fraud_email.csv')\n",
        "# get the spam_list\n",
        "spam_list = fraud_email[(fraud_email['Class'] == 1)]['Text'].tolist()\n",
        "# get the ham_list\n",
        "ham_list = fraud_email[(fraud_email['Class'] == 0)]['Text'].tolist()\n",
        "# get the numbers of instances\n",
        "dataset_row_count = len(fraud_email.axes[0])\n",
        "# get the probabilities of ham/spam mail in this dataset\n",
        "prob_ham_in_dataset = len(ham_list)/dataset_row_count\n",
        "prob_spam_in_dataset = len(spam_list)/dataset_row_count\n",
        "print('setup and initialization are completed!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "setup and initialization are completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExSSyAhHjGzm"
      },
      "source": [
        "Write down how to extract words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GFjZ5UyjGzo"
      },
      "source": [
        "def extract_words(text):\n",
        "    split_list = re.split('; |;|, |,|: |:|\\t|\\*|\\n|! | |\\.|\\. ', str(text))\n",
        "    word_list = [word.lower() for word in split_list]\n",
        "    word_list = list(dict.fromkeys(word_list))\n",
        "    if str('') in word_list:\n",
        "        word_list.remove('')\n",
        "    return word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gsIjFEKjGzq"
      },
      "source": [
        "How to calculate the probability of occurence od a word in a given text list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz70ymbDjGzr"
      },
      "source": [
        "def prob_in_set(word, list):\n",
        "    count = 0\n",
        "    for mail in list:\n",
        "        if word in str(mail).lower():\n",
        "            count += 1\n",
        "    return count/len(list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpRupjdQjGzt"
      },
      "source": [
        "This is how to calculate the \"Pi<i = 1 -> n>(P(w_i|spam))\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GAj0-hwjGzv"
      },
      "source": [
        "def product_of_num_list(num_list):\n",
        "    result = 1\n",
        "    for num in num_list:\n",
        "        result *= float(num)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttn7wFpIjGzw"
      },
      "source": [
        "Now we can start calculating an instance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqZFy71tjGzy"
      },
      "source": [
        "def main(text):\n",
        "    word_list = extract_words(text)\n",
        "    prob_in_spam_list = list()\n",
        "    prob_in_ham_list = list()\n",
        "    for word in word_list:\n",
        "        prob_in_spam_list.append(prob_in_set(word, spam_list))\n",
        "        prob_in_ham_list.append(prob_in_set(word, ham_list))\n",
        "    ratio = (product_of_num_list(prob_in_ham_list) * prob_ham_in_dataset) / \\\n",
        "        (product_of_num_list(prob_in_spam_list) * prob_spam_in_dataset)\n",
        "    print('Ratio: ' + str(ratio))\n",
        "    prob = 1 / (1 + ratio)\n",
        "    print(\"You email has a chance of \" + str(prob*100) + \"% of being a spam.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj2mMq81jGz0"
      },
      "source": [
        "Let's try it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekc6XUvJjGz2"
      },
      "source": [
        "text = \"I'm just a little bit caught in the middle\\\n",
        "Life is a maze and love is a riddle\\\n",
        "I don't know where to go; can't do it alone; I've tried\\\n",
        "And I don't know why\\\n",
        "Slow it down\\\n",
        "Make it stop\\\n",
        "Or else my heart is going to pop\\\n",
        "\\'Cause it\\'s too much\\\n",
        "Yeah, it's a lot\\\n",
        "To be something I'm not\"\n",
        "main(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRuI9mJAjGz4"
      },
      "source": [
        "Can you see that there is the division by zero problem and the model is overfit that the predict result will always be around 99%? I have fixed this like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFZgmnNZjGz6"
      },
      "source": [
        "def product_of_num_list(num_list):\n",
        "    result = 1\n",
        "    for num in num_list:\n",
        "        result *= float(num + 10)\n",
        "    return result\n",
        "# 10 is the number added due to Laplace correction to avoid the 0.0 probability.\n",
        "# But why is it 10 instead of another number?\n",
        "# I do not really know, I just adjust the number due to the test result.\n",
        "# If it's too big, the return number will be infinity, but if it's too small,\n",
        "# the model is overfit :(.\n",
        "# Sorry!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc233JSFjGz8"
      },
      "source": [
        "Now let's try again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdV7OQ78jGz9",
        "outputId": "776c3c4d-be88-44d7-e573-bea860346ec1"
      },
      "source": [
        "text = \"I'm just a little bit caught in the middle\\\n",
        "Life is a maze and love is a riddle\\\n",
        "I don't know where to go; can't do it alone; I've tried\\\n",
        "And I don't know why\\\n",
        "Slow it down\\\n",
        "Make it stop\\\n",
        "Or else my heart is going to pop\\\n",
        "\\'Cause it\\'s too much\\\n",
        "Yeah, it's a lot\\\n",
        "To be something I'm not\"\n",
        "main(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ratio: 0.6714713818373087\n",
            "You email has a chance of 59.827527462706755% of being a spam.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t58rXksIjG0A"
      },
      "source": [
        "But we have another problem to notice: there's a lot of words like \"a\", \"the\", \"is\", \"it\",...etc., and I think it's too trivial to take into account in this model, so I will remove it by this function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txlxxyLkjG0C"
      },
      "source": [
        "def idf(word):\n",
        "    count = 0\n",
        "    for mail in fraud_email['Text'].tolist():\n",
        "        if word in str(mail).lower():\n",
        "            count += 1\n",
        "    return math.log(1 + (dataset_row_count/(count + 1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0vDByIVjG0E"
      },
      "source": [
        "The main() function will be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtBrKkOOjG0F"
      },
      "source": [
        "def main(text):\n",
        "    word_list = extract_words(text)\n",
        "    prob_in_spam_list = list()\n",
        "    prob_in_ham_list = list()\n",
        "    for word in word_list:\n",
        "        if idf(word) > 1:\n",
        "            prob_in_spam_list.append(prob_in_set(word, spam_list))\n",
        "            prob_in_ham_list.append(prob_in_set(word, ham_list))\n",
        "    ratio = (product_of_num_list(prob_in_ham_list) * prob_ham_in_dataset) / \\\n",
        "        (product_of_num_list(prob_in_spam_list) * prob_spam_in_dataset)\n",
        "    print('Ratio: ' + str(ratio))\n",
        "    prob = 1 / (1 + ratio)\n",
        "    print(\"You email has a chance of \" + str(prob*100) + \"% of being a spam.\")\n",
        "    return (0 if (prob < 0.65) else 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5dQ0eL8jG0I"
      },
      "source": [
        "Now we have a complete simple spam filter. Because the speed of testing an entire dataset is too slow (:<) so will try a piece of instances here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZO9i7F0jG0K",
        "outputId": "dc6bcb09-8751-450f-b4f9-dcf02dbe09bb"
      },
      "source": [
        "start = 1000\n",
        "stop = 1020\n",
        "sample = fraud_email['Text'].tolist()[start:stop]\n",
        "sample_result = fraud_email['Class'].tolist()[start:stop]\n",
        "test_result = list()\n",
        "for mail in sample:\n",
        "    test_result.append(main(str(mail)))\n",
        "print(\"Sample result: \" + str(sample_result))\n",
        "print(\"Test result: \" + str(test_result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ratio: nan\n",
            "You email has a chance of nan% of being a spam.\n",
            "Ratio: 0.5417626251102475\n",
            "You email has a chance of 64.86082771195032% of being a spam.\n",
            "Ratio: 0.01333739730200361\n",
            "You email has a chance of 98.68381475533083% of being a spam.\n",
            "Ratio: 0.026624087640040428\n",
            "You email has a chance of 97.40663715564644% of being a spam.\n",
            "Ratio: 0.18698210028417106\n",
            "You email has a chance of 84.2472687465627% of being a spam.\n",
            "Ratio: 0.02265880584114123\n",
            "You email has a chance of 97.78432398843871% of being a spam.\n",
            "Ratio: 0.76348771554806\n",
            "You email has a chance of 56.7058103769789% of being a spam.\n",
            "Ratio: 1.3138832223547976\n",
            "You email has a chance of 43.21739275080261% of being a spam.\n",
            "Ratio: 0.7997538386148971\n",
            "You email has a chance of 55.5631541683282% of being a spam.\n",
            "Ratio: 1.2298480006601815\n",
            "You email has a chance of 44.84610608902195% of being a spam.\n",
            "Ratio: 0.40413926770230413\n",
            "You email has a chance of 71.218006860272% of being a spam.\n",
            "Ratio: 0.99681744675709\n",
            "You email has a chance of 50.0796906409266% of being a spam.\n",
            "Ratio: 1.2665345646240698\n",
            "You email has a chance of 44.1202183989575% of being a spam.\n",
            "Ratio: 1.0599127117443723\n",
            "You email has a chance of 48.54574634636734% of being a spam.\n",
            "Ratio: 0.633399230204453\n",
            "You email has a chance of 61.222019792113514% of being a spam.\n",
            "Ratio: 0.7475819062221596\n",
            "You email has a chance of 57.22192455984813% of being a spam.\n",
            "Ratio: nan\n",
            "You email has a chance of nan% of being a spam.\n",
            "Ratio: 0.8007222868208576\n",
            "You email has a chance of 55.53327169429784% of being a spam.\n",
            "Ratio: 0.010518351525091112\n",
            "You email has a chance of 98.95911326011877% of being a spam.\n",
            "Ratio: 0.006173827381185215\n",
            "You email has a chance of 99.38640548847765% of being a spam.\n",
            "Sample result: [1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
            "Test result: [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhB_rGcZjG0N"
      },
      "source": [
        "It's not enough to conclude the accuracy based on this small test. But it's quite optimistic, right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVu4bVfSjG0P"
      },
      "source": [
        "This is the end. Thank you for reading my notebook. I also want to thank my friends who help and propose ways to reduce overfit and solve other problems in this project."
      ]
    }
  ]
}